---
layout: post
title: "【喂饭级教程】llama3 fine-tuning｜ 使用GGUF 和llama.cpp 量化Llama模型｜GPT4All接入量化模型，实现CPU推理聊天"
date: 2024-05-03 16:00:00
subtitle: "中文微调Llama3"
author: "lingyun0406"
header-style: text
catalog: true
tags:
  - llama3
  - fine-tuning
  - unslo
  - CUDA
  - cuDNN
  - Python
  - Visual Studio 2022
  - HuggingFace
  - git
  - deepspeed
  - triton
  - xformers
  - bitsandbytes
---


> 依赖软件：`CUDA12.1+cuDNN8.9`、`Python 3.11.9`、`Git`、`Visual Studio 2022`

# 详细部署步骤 ：

### 一、下载安装包

1. 解压`unsloth`
2. 安装`python 3.11`
3. 安装`cuda12.1`，配置`cuDNN8.9` 
4. 安装`Visual Studio 2022`
5. 安装`gpt4all`
6. 安装`git`


### 二、设置环境变量

1. 设置CUDA路径到系统环境变量path里 
   `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin`

2. 设置CUDA Compiler环境变量：

   新增系统环境变量，变量名：`CC`  变量值：`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin\nvcc.exe`

3. 设置MSVC 编译器路径到系统环境变量path里

   `C:\Program Files\Microsoft Visual Studio\2022\Professional\VC\Tools\MSVC\14.39.33519\bin\Hostx64\x64`

4. 设置Visual Studio 2022中cmake路径到系统环境变量path里

   `C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin`

   `C:\Program Files\Microsoft Visual Studio\2022\Professional`

### 三、安装unsloth依赖

1. 使用python3.11创建虚拟环境 

   ```
   python311\python.exe -m venv venv 
   ```

2. 激活虚拟环境

   ```
    call python311\venv\Scripts\activate.bat
   ```

3. 安装依赖包 

   ```
   pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu121
   pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git
   pip install --no-deps trl peft accelerate bitsandbytes 
   pip install deepspeed-0.13.1+unknown-py3-none-any.whl 
   pip install triton-2.1.0-cp311-cp311-win_amd64.whl 
   pip install xformers=v0.0.25.post1
   ```

4. 测试安装是否成功

   ```
   nvcc  --version 
   python -m xformers.info 
   python -m bitsandbytes 
   ```

###  四、微调推理

- 运行脚本 `test-unlora.py` 测试微调之前推理 
- 运行脚本`fine-tuning.py`用数据集微调  
- 运行脚本`test-lora.py`测试微调之后推理  

###  五、4位量化模型：

1. `git clone https://github.com/ggerganov/llama.cpp`

2. 按官方文档编译

   ```
   mkdir build 
   cd build 
   cmake .. -DLLAMA_CUBLAS=ON 
   ```

3. 编译`llama.cpp cmake --build . --config Release` 

   <!--如果上面这句编译命令无法执行，需要做以下操作： 复制这个路径下的 C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\extras\visual_studio_integration\MSBuildExtensions 4个文件，粘贴到以下目录里 C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\MSBuild\Microsoft\VC\v170\BuildCustomizations--> 

4. 编译好以后，把`llama.cpp\build\bin\Release`目录下的所有文件复制到`llama.cpp`目录下 

5. 重新运行`python fine-tuning.py`微调保存4位量化模型
